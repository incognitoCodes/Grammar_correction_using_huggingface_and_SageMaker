{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56dae349",
   "metadata": {},
   "source": [
    "## Deploying a Hugging Face model in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625b5e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SageMaker -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f205a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2c5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'vennify/t5-base-grammar-correction',\n",
    "  'HF_TASK':'text2text-generation'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc5927",
   "metadata": {},
   "source": [
    "## Defining the HuggingFace Model and deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeb5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.17.0',\n",
    "    pytorch_version='1.10.2',\n",
    "    py_version='py38',\n",
    "    env=hub,\n",
    "    role=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec04e1e",
   "metadata": {},
   "source": [
    "Now, let's test our model with few sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61715191",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I will go not home today'\n",
    "#text = 'he like to eat noodles'\n",
    "#text = 'Once upon a time, there is a king with a big empire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2521d282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I will not go home today.'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sent = predictor.predict({\"inputs\": \"grammar:\"+ text})\n",
    "corrected_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce7faf6",
   "metadata": {},
   "source": [
    "Now, let's test with a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b835b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"On cold, wet morning, my class was filled with excitement. Someone have discover that the next day was our teacher's birthday. Our teacher was the kindest person that ever exist. Thus it is no surprise she was the favourite teacher to the pupils. Everyone want to get her a present. I, very much wanted to shown any appreciation too. That afternoon, I spends the whole afternoon shop for a present. After a long search, I finally made on my mind. The next day I gived her a bouquet of beautiful roses and she exclaimed with pleasure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ca8d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On cold, wet morning, my class was filled with excitement\n",
      " Someone have discover that the next day was our teacher's birthday\n",
      " Our teacher was the kindest person that ever exist\n",
      " Thus it is no surprise she was the favourite teacher to the pupils\n",
      " Everyone want to get her a present\n",
      " I, very much wanted to shown any appreciation too\n",
      " That afternoon, I spends the whole afternoon shop for a present\n",
      " After a long search, I finally made on my mind\n",
      " The next day I gived her a bouquet of beautiful roses and she exclaimed with pleasure\n"
     ]
    }
   ],
   "source": [
    "for lines in paragraph.split(\".\"):\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "706dfaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'On a cold, wet morning, my class was filled with excitement.'}]\n",
      "[{'generated_text': \"Someone has discovered that the next day was our teacher's birthday.\"}]\n",
      "[{'generated_text': 'Our teacher was the kindest person that ever existed.'}]\n",
      "[{'generated_text': 'Thus it is no surprise that she was the favourite teacher to the pupils.'}]\n",
      "[{'generated_text': 'Everyone wanted to get her a present.'}]\n",
      "[{'generated_text': 'I, very much wanted to show any appreciation too.'}]\n",
      "[{'generated_text': 'That afternoon, I spend the whole afternoon shopping for a present.'}]\n",
      "[{'generated_text': 'After a long search, I finally made up my mind.'}]\n",
      "[{'generated_text': 'The next day I gave her a bouquet of beautiful roses and she exclaimed with pleasure'}]\n"
     ]
    }
   ],
   "source": [
    "for lines in paragraph.split(\".\"):\n",
    "    corrected_sent = predictor.predict({\"inputs\": \"grammar:\"+ lines})\n",
    "    print(corrected_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b747ad",
   "metadata": {},
   "source": [
    "## Deploying serverless inference in SageMaker\n",
    "\n",
    "Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers. Serverless Inference integrates with AWS Lambda to offer you high availability, built-in fault tolerance and automatic scaling.\n",
    "\n",
    "With a pay-per-use model, Serverless Inference is a cost-effective option if you have an infrequent or unpredictable traffic pattern. During times when there are no requests, Serverless Inference scales your endpoint down to 0, helping you to minimize your costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfea9a5",
   "metadata": {},
   "source": [
    "### Endpoint Configuration Creation\n",
    "\n",
    "This is where you can adjust the Serverless Configuration for your endpoint. You will define the max concurrent invocations for a single endpoint, known as **MaxConcurrency**, and the **Memory size**. \n",
    "\n",
    "Your serverless endpoint has a minimum RAM size of 1024 MB (1 GB), and the maximum RAM size you can choose is 6144 MB (6 GB). **The memory sizes you can choose are 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, or 6144 MB**. Serverless Inference auto-assigns compute resources proportional to the memory you select. If you choose a larger memory size, your container has access to more vCPUs. \n",
    "\n",
    "Serverless endpoints have a quota for how many concurrent invocations can be processed at the same time. If the endpoint is invoked before it finishes processing the first request, then it handles the second request concurrently. **You can set the maximum concurrency for a single endpoint up to 200, and the total number of serverless endpoints you can host in a Region is 50**. The maximum concurrency for an individual endpoint prevents that endpoint from taking up all of the invocations allowed for your account, and any endpoint invocations beyond the maximum are throttled.\n",
    "\n",
    "If your endpoint does not receive traffic for a while and then your endpoint suddenly receives new requests, it can take some time for your endpoint to spin up the compute resources to process the requests. This is called a **cold start**. Since serverless endpoints provision compute resources on demand, your endpoint may experience cold starts. A cold start can also occur if your concurrent requests exceed the current concurrent request usage. The cold start time depends on your model size, how long it takes to download your model, and the start-up time of your container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca2e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
