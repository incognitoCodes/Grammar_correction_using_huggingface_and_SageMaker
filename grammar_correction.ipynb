{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d9f6ae",
   "metadata": {},
   "source": [
    "## Deploying a Hugging Face model in SageMaker\n",
    "\n",
    "In this notebook, we will practice deploying a Hugging Face pre-trained model from Hugging face hub. \n",
    "\n",
    "We will use a pre-trained model for grammar correction. The model that we will use can be found [here](https://huggingface.co/vennify/t5-base-grammar-correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install SageMaker -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b406f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary libraries and defining the role\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import ast\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa3daa",
   "metadata": {},
   "source": [
    "**To deploy a model directly from the Hugging Face Model Hub to Amazon SageMaker, we need to define two environment variables when creating the HuggingFaceModel.**\n",
    "\n",
    "We need to define:\n",
    "\n",
    "- HF_MODEL_ID: defines the model id, which will be automatically loaded from huggingface.co/models when creating or SageMaker Endpoint. \n",
    "- HF_TASK: defines the task for the used ðŸ¤— Transformers pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be1d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'vennify/t5-base-grammar-correction',\n",
    "  'HF_TASK':'text2text-generation'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9c69b",
   "metadata": {},
   "source": [
    "## Defining the HuggingFace Model and deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5149536f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.17.0',\n",
    "    pytorch_version='1.10.2',\n",
    "    py_version='py38',\n",
    "    env=hub,\n",
    "    role=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type='ml.m5.xlarge' # ec2 instance type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c316f5",
   "metadata": {},
   "source": [
    "Now, let's test our model with few sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f03955e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I will go not home today'\n",
    "text = 'Our teacher was the kind person that ever existed.'\n",
    "#text = 'Once upon a time, there is a king with a big empire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa1b9461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Our teacher was the kindest person that ever existed.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sent = predictor.predict({\"inputs\": \"grammar:\"+ text})\n",
    "corrected_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236438f",
   "metadata": {},
   "source": [
    "Now, let's test with a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd399cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"On cold, wet morning, my class was filled with excitement. Someone have discover that the next day was our teacher's birthday. Our teacher was the kindest person that ever exist. Thus it is no surprise she was the favourite teacher to the pupils. Everyone want to get her a present. I, very much wanted to shown any appreciation too. That afternoon, I spends the whole afternoon shop for a present. After a long search, I finally made on my mind. The next day I gived her a bouquet of beautiful roses and she exclaimed with pleasure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a8e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please note that for larger text, you need to add a parameter called max_length so that the model can understand that it needs to do for entire text\n",
    "\n",
    "payload = {\"inputs\": \"grammar:\" + paragraph, \n",
    "               \"parameters\" : {\n",
    "                            \"max_length\": int(len(paragraph) + 10)\n",
    "  }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd27c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a cold, wet morning, my class was filled with excitement. Someone discovered that the next day was our teacher's birthday. Our teacher was the kindest person that ever existed. Thus it is no surprise that everyone wanted to give her a present. I, very much wanted to show appreciation too. That afternoon, I spent the whole afternoon shopping for a present. After a long search, I finally made my mind. The next day I gave her a bouquet of beautiful roses and she exclaimed with pleasure.\n"
     ]
    }
   ],
   "source": [
    "corrected_sent = predictor.predict(payload)\n",
    "print(corrected_sent[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3caca9",
   "metadata": {},
   "source": [
    "## Deploying serverless inference in SageMaker\n",
    "\n",
    "Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers. Serverless Inference integrates with AWS Lambda to offer you high availability, built-in fault tolerance and automatic scaling.\n",
    "\n",
    "With a pay-per-use model, Serverless Inference is a cost-effective option if you have an infrequent or unpredictable traffic pattern. During times when there are no requests, Serverless Inference scales your endpoint down to 0, helping you to minimize your costs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487590a5",
   "metadata": {},
   "source": [
    "### Endpoint Configuration Creation\n",
    "\n",
    "This is where you can adjust the Serverless Configuration for your endpoint. You will define the max concurrent invocations for a single endpoint, known as **MaxConcurrency**, and the **Memory size**. \n",
    "\n",
    "Your serverless endpoint has a minimum RAM size of 1024 MB (1 GB), and the maximum RAM size you can choose is 6144 MB (6 GB). **The memory sizes you can choose are 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, or 6144 MB**. Serverless Inference auto-assigns compute resources proportional to the memory you select. If you choose a larger memory size, your container has access to more vCPUs. \n",
    "\n",
    "Serverless endpoints have a quota for how many concurrent invocations can be processed at the same time. If the endpoint is invoked before it finishes processing the first request, then it handles the second request concurrently. **You can set the maximum concurrency for a single endpoint up to 200, and the total number of serverless endpoints you can host in a Region is 50**. The maximum concurrency for an individual endpoint prevents that endpoint from taking up all of the invocations allowed for your account, and any endpoint invocations beyond the maximum are throttled.\n",
    "\n",
    "If your endpoint does not receive traffic for a while and then your endpoint suddenly receives new requests, it can take some time for your endpoint to spin up the compute resources to process the requests. This is called a **cold start**. Since serverless endpoints provision compute resources on demand, your endpoint may experience cold starts. A cold start can also occur if your concurrent requests exceed the current concurrent request usage. The cold start time depends on your model size, how long it takes to download your model, and the start-up time of your container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d34a1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sagemaker.serverless.serverless_inference_config import ServerlessInferenceConfig\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# Define SageMaker client and SageMaker runtime\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the model name from the SageMaker console. The model name will start with huggingface-pytorch-inference*\n",
    "\n",
    "model_name='huggingface-pytorch-inference-2022-11-22-09-57-17-455'\n",
    "rcf_serverless_config = 'hf-serverless-grammarcorrection-epc'+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# Serverless Endpoint config creation using the API \"create_endpoint_config\"\n",
    "\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=rcf_serverless_config,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"HFServerlessVariant\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"ServerlessConfig\": {\n",
    "                \"MemorySizeInMB\": 4096,\n",
    "                \"MaxConcurrency\": 1,\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a70bf",
   "metadata": {},
   "source": [
    "### Creating the serverless endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82641d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"HF-serverless-grammar-correction\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=rcf_serverless_config,\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec10c4",
   "metadata": {},
   "source": [
    "### Invoking the serverless endpoint using boto3 SageMaker runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30038c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Our teacher was the kind person that ever existed\"\n",
    "payload = json.dumps({\"inputs\": \"grammar:\"+ text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "570913a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 3.59 ms, total: 14.2 ms\n",
      "Wall time: 23.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Our teacher was the kindest person that ever existed.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Get the endpoint name from the SageMaker console under \"Inference\" > \"Endpoints\"\n",
    "\n",
    "endpoint_name='HF-serverless-grammar-correction2022-11-23-01-09-25'\n",
    "\n",
    "## Let's invoke the endpoint to get the response \n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                       ContentType='application/json',\n",
    "                                       Body=payload)\n",
    "text_op = response ['Body'].read().decode()\n",
    "\n",
    "# convert string output to array and get the corrected sentence \n",
    "text_corrected = ast.literal_eval(text_op)\n",
    "text_corrected[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf7392",
   "metadata": {},
   "source": [
    "Now, let's test the same paragraph again using serverless endpoint to make sure that we get the same response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ffb64c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"On cold, wet morning, my class was filled with excitement. Someone have discover that the next day was our teacher's birthday. Our teacher was the kindest person that ever exist. Thus it is no surprise she was the favourite teacher to the pupils. Everyone want to get her a present. I, very much wanted to shown any appreciation too. That afternoon, I spends the whole afternoon shop for a present. After a long search, I finally made on my mind. The next day I gived her a bouquet of beautiful roses and she exclaimed with pleasure\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "652680d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"inputs\": \"grammar:\" + paragraph, \n",
    "               \"parameters\" : {\n",
    "                            \"max_length\": int(len(paragraph) + 10)\n",
    "  }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aab8b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On a cold, wet morning, my class was filled with excitement. Someone discovered that the next day was our teacher's birthday. Our teacher was the kindest person that ever existed. Thus it is no surprise that everyone wanted to give her a present. I, very much wanted to show appreciation too. That afternoon, I spent the whole afternoon shopping for a present. After a long search, I finally made my mind. The next day I gave her a bouquet of beautiful roses and she exclaimed with pleasure.\n"
     ]
    }
   ],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=json.dumps(payload))\n",
    "\n",
    "text_op = response['Body'].read().decode()\n",
    "\n",
    "text_corrected = ast.literal_eval(text_op)\n",
    "print(text_corrected[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b4742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
